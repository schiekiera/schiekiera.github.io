---
layout: post
date: 2026-02-03 12:00:00+0100
inline: true
related_posts: false
---

Excided to share a new preprint exploring how much we can infer about LLMs' internal semantic structure from behavioral observations alone. We tested eight transformer models using psycholinguistic paradigms (free association and forced choice) on 5,000 words. We found that forced-choice tasks better recover internal semantic geometry than free association, and behavioral measurements predict hidden-state similarities beyond baseline methods. 


**Preprint:** [arXiv:2602.00628 (PDF)](https://www.arxiv.org/pdf/2602.00628).