---
layout: post
date: 2026-02-03 12:00:00+0100
inline: true
related_posts: false
---

Excited to share a new preprint on a classic cognitive-science question: *if you only observe a system’s behavior, how much can you infer about the structure of its internal (latent) semantic state?* Across eight instruction-tuned transformer LLMs, we run large-scale psycholinguistic paradigms—free association and similarity-based forced choice—over the same 5,000-word vocabulary (17.5M+ trials) and compare the resulting behavior-derived semantic geometry to layer-wise hidden-state similarity via representational similarity analysis. We find that forced-choice judgments recover internal semantic geometry substantially better than free association, and that behavior-only measurements can predict unseen hidden-state similarities beyond lexical baselines and cross-model consensus—suggesting constrained behavioral probes can be surprisingly informative even under black-box access.

**Preprint:** [arXiv:2602.00628 (PDF)](https://www.arxiv.org/pdf/2602.00628).