<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs | Louis Schiekiera </title> <meta name="author" content="Louis Schiekiera"> <meta name="description" content="Our preprint &lt;b&gt;From Associations to Activations&lt;/b&gt; investigates whether an LLM's internal semantic geometry can be recovered from its observable behavior. Across eight instruction-tuned transformers and 17.5M+ trials, we compare behavior-derived similarity structures from forced-choice and free-association paradigms to layerwise hidden-state geometry using representational similarity analysis. We find that forced-choice behavior aligns substantially more with internal representations than free association, and that behavioral similarity predicts unseen hidden-state similarities beyond lexical baselines."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%97&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://schiekiera.github.io/blog/2026/neural-semantic-geometry/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs",
            "description": "Our preprint <b>From Associations to Activations</b> investigates whether an LLM's internal semantic geometry can be recovered from its observable behavior. Across eight instruction-tuned transformers and 17.5M+ trials, we compare behavior-derived similarity structures from forced-choice and free-association paradigms to layerwise hidden-state geometry using representational similarity analysis. We find that forced-choice behavior aligns substantially more with internal representations than free association, and that behavioral similarity predicts unseen hidden-state similarities beyond lexical baselines.",
            "published": "February 16, 2026",
            "authors": [
              
              {
                "author": "Louis Schiekiera",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Humboldt-Universität zu Berlin & Freie Universität Berlin",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Max Zimmer",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Zuse Institute Berlin & Technische Universität Berlin",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Christophe Roux",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Zuse Institute Berlin & Technische Universität Berlin",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sebastian Pokutta",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Zuse Institute Berlin & Technische Universität Berlin",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Fritz Günther",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Humboldt-Universität zu Berlin",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Louis </span> Schiekiera </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs</h1> <p>Our preprint <b>From Associations to Activations</b> investigates whether an LLM's internal semantic geometry can be recovered from its observable behavior. Across eight instruction-tuned transformers and 17.5M+ trials, we compare behavior-derived similarity structures from forced-choice and free-association paradigms to layerwise hidden-state geometry using representational similarity analysis. We find that forced-choice behavior aligns substantially more with internal representations than free association, and that behavioral similarity predicts unseen hidden-state similarities beyond lexical baselines.</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="motivation-can-behavior-reveal-internal-structure">Motivation: Can behavior reveal internal structure?</h2> <p>In cognitive science, semantic knowledge is treated as a latent structure: we cannot observe a speaker’s meaning representation directly, but we can systematically probe it through behavior <d-cite key="de2019small"></d-cite>. Word-association paradigms use exactly this logic—when a participant sees a cue (e.g., <em>dog</em>), the associations they produce or select (e.g., <em>cat</em>, <em>leash</em>, <em>bark</em>) are constrained by their underlying semantic organization. When such judgments are aggregated across trials, the resulting response statistics yield a similarity matrix that approximates the geometry of an otherwise unobserved semantic system.</p> <p>We transfer this measurement logic to large language models. Unlike humans, both behavior <em>and</em> internal representations are observable in LLMs. This creates a unique opportunity: we can systematically test how well an LLM’s behavioral output reveals its internal semantic geometry. A key open question is not only how model behavior compares to humans, but also what a model’s <em>own</em> behavior reveals about its <em>own</em> internal representations.</p> <div class="figure-container"> <img src="/assets/img/blog/neural_semantic_geometry/conceptual.pdf" alt="Conceptual overview of the framework" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Conceptual overview. For a shared vocabulary, we (i) extract layer-wise word representations to form a hidden-state similarity matrix, and (ii) run behavioral association tasks (forced choice / free association) to build a behavioral similarity matrix. Representational similarity analysis (RSA) correlates the pairwise similarities to quantify behavior--activation alignment.</div> </div> <h2 id="framework-behavioral-paradigms-and-hidden-state-extraction">Framework: Behavioral paradigms and hidden-state extraction</h2> <h3 id="two-behavioral-paradigms">Two behavioral paradigms</h3> <p>We use two classic psycholinguistic paradigms—forced choice (FC) and free association (FA)—to collect semantic relations from model behavior over a shared vocabulary of 5,000 high-frequency English nouns <d-cite key="brysbaert2012adding"></d-cite>.</p> <div class="figure-container"> <img src="/assets/img/blog/neural_semantic_geometry/both_paradigms.pdf" alt="Forced choice and free association paradigms" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Behavioral paradigms and derived semantic geometries. Left (forced choice): given a cue word and a candidate set, the model selects the most related words. Right (free association): given a cue word alone, the model generates multiple associates. From the resulting cue--response count matrices, we compute similarity matrices by cosine similarity between rows.</div> </div> <p>In the <strong>forced-choice</strong> paradigm, each cue word is presented together with 16 candidate words, from which the model must select exactly two words that are most semantically related to the cue. Candidate sets are constructed by a deterministic shuffle of the remaining vocabulary, yielding 313 FC trials per cue. In the <strong>free-association</strong> paradigm, the model is prompted with a single cue word and asked to generate exactly five single-word associates. This is repeated across 126 stochastic runs per cue.</p> <p>For each paradigm, model outputs are aggregated into a sparse cue–response count matrix $\mathbf{B}$. We reweight counts with positive pointwise mutual information (PPMI) to reduce the influence of globally frequent responses, then compute a cue–cue similarity matrix via cosine similarity between the PPMI-weighted row vectors. In total, we collected over <strong>17.5 million trials</strong> across both paradigms and eight models.</p> <h3 id="hidden-state-extraction">Hidden-state extraction</h3> <p>For each model and each word, we extract layerwise hidden-state representations under four contextual embedding strategies:</p> <ul> <li> <strong>Averaged</strong>: The target word embedded in 50 naturally occurring C4 sentences <d-cite key="raffel2020exploring"></d-cite>, with hidden states averaged across contexts <d-cite key="bommasani2020interpreting"></d-cite>.</li> <li> <strong>Meaning</strong>: A fixed definition-style prompt (<em>“What is the meaning of the word {w}?”</em>).</li> <li> <strong>Task (FC)</strong>: The target word embedded in the FC instruction prompt (without candidates).</li> <li> <strong>Task (FA)</strong>: The target word embedded in the FA instruction prompt.</li> </ul> <p>Hidden-state similarity matrices are computed as cosine similarity between mean-centered layerwise word vectors <d-cite key="ethayarajh2019contextual"></d-cite>.</p> <h3 id="models-and-baselines">Models and baselines</h3> <p>We evaluate eight instruction-tuned decoder-only transformer models ranging from 7B to 14B parameters (Falcon3, Gemma-2, Llama-3.1, Mistral-7B, Mistral-Nemo, Phi-4, Qwen2.5, and rnj-1). Beyond behavioral embeddings, we compare hidden-state similarities to three baselines: <strong>FastText</strong> (static word vectors) <d-cite key="bojanowski2017enriching"></d-cite>, <strong>BERT</strong> (contextual encoder) <d-cite key="devlin2019bert"></d-cite>, and a <strong>cross-model consensus</strong> geometry aggregating hidden-state similarities across all other models—motivated by recent evidence for a shared semantic subspace across diverse LLMs <d-cite key="huh2024platonic"></d-cite>.</p> <h3 id="evaluation">Evaluation</h3> <p>We use three complementary evaluation methods:</p> <ol> <li> <p><strong>Representational Similarity Analysis (RSA)</strong> <d-cite key="kriegeskorte2008representational"></d-cite> <d-cite key="nili2014toolbox"></d-cite>: For each layer, we vectorize the upper-triangular entries of the hidden-state and reference similarity matrices and compute their Pearson correlation.</p> </li> <li> <p><strong>Nearest-neighbor overlap</strong> ($\mathrm{NN@}k$): We quantify how well the $k$-nearest-neighbor neighborhoods induced by hidden-state similarity match those of the reference spaces.</p> </li> <li> <p><strong>Held-out-words ridge regression</strong>: We test whether behavioral similarity predicts unseen hidden-state similarities on held-out words beyond lexical baselines and cross-model consensus.</p> </li> </ol> <h2 id="results">Results</h2> <h3 id="forced-choice-aligns-substantially-more-than-free-association">Forced choice aligns substantially more than free association</h3> <p>Across all models and evaluation methods, FC behavior aligns substantially more strongly with hidden-state geometry than FA. Mean FC RSA increases from $r = .346$ under Averaged extraction to $r = .463$ under Task (FC), while FA shows the same pattern at considerably lower magnitude ($r = .140$ to $r = .199$).</p> <div class="figure-container"> <img src="/assets/img/blog/neural_semantic_geometry/rsa_nn_grid_1x2.pdf" alt="Summary RSA and nearest-neighbor overlap results" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Summary of RSA and neighborhood-overlap results (means across models). Left: mean RSA Pearson correlation as a function of layer. Right: mean nearest-neighbor overlap as a function of neighborhood size $k$ (log scale). FC behavior (green) aligns substantially more with hidden-state geometry than FA behavior (red), while cross-model consensus (black) provides the strongest reference.</div> </div> <p>Task-aligned and meaning-based extraction strategies yield the strongest alignment at earlier, mid-depth layers, whereas averaging over natural contexts shifts alignment peaks to later layers.</p> <div class="figure-container"> <img src="/assets/img/blog/neural_semantic_geometry/rsa_line_plot_1x2_grid_fc_fa.pdf" alt="Layerwise RSA for FC and FA under different extraction strategies" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Layerwise RSA for PPMI-weighted forced-choice similarity (left) and free-association similarity (right) under different extraction strategies. Task-aligned prompts yield peak alignment at earlier layers, while averaged contexts shift peaks later.</div> </div> <p>The full model-by-model RSA comparison reveals that the FC advantage is consistent across all eight models, though the magnitude varies:</p> <div class="figure-container"> <img src="/assets/img/blog/neural_semantic_geometry/rsa_fc_fa_2x4_grid.pdf" alt="RSA heatmap across models" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">RSA between model hidden-state similarity and behavior-derived semantic geometries. Each panel corresponds to a model and compares hidden-state similarity to PPMI-weighted forced-choice (left) and free-association (right) behavioral embeddings across extraction strategies and layers.</div> </div> <h3 id="behavioral-similarity-predicts-unseen-hidden-state-structure">Behavioral similarity predicts unseen hidden-state structure</h3> <p>The held-out-words ridge regression shows that behavioral similarity—especially FC—predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus. Adding behavioral FC similarity on top of the baseline improves mean test $R^2$ by $+.022$, whereas FA yields a smaller gain ($+.002$). The full model reaches mean $R^2 = .587$ (vs. $.569$ for the baseline). Peak performance reaches $R^2 = .844$ for Llama-3.1-8B-Instruct.</p> <div class="figure-container"> <img src="/assets/img/blog/neural_semantic_geometry/rr_model_performance_grid_2x4.pdf" alt="Ridge regression performance across models" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Ridge regression performance for predicting hidden-state similarity from behavioral and lexical features across eight models. Bold values show $R^2$ for the full model (behavioral + baselines); parenthetical values show the baseline without behavioral features.</div> </div> <h2 id="discussion-and-implications">Discussion and implications</h2> <p>Our findings show that structured behavior—particularly from constrained measurement paradigms like forced choice—preserves a nontrivial projection of a model’s hidden-state similarity geometry, even without access to logits or internal activations. This has implications for both interpretability research and cognitive science:</p> <p><strong>For interpretability</strong>: Behavioral probing can serve as a practical tool for understanding internal representations when only black-box access is available. The FC paradigm’s controlled candidate sets concentrate observations and produce a less sparse cue–response matrix, yielding higher signal-to-noise measurements of semantic geometry <d-cite key="roads2021enriching"></d-cite>.</p> <p><strong>For cognitive science</strong>: Using our fully observable language-model setup, we can subject a core assumption to rigorous empirical tests—that structured behavior is constrained by, and can therefore partially reveal, internal states. The finding that measurement protocol strongly determines recoverability (FC vs. FA) suggests that whether a behavioral task <em>reveals</em> internal structure is not a generic property of “behavior”: it depends critically on how responses are constrained and aggregated.</p> <p><strong>Cross-model consensus</strong>: A striking finding is the strength of cross-model consensus—similarity structure shared across other LLMs explains a large fraction of variance in a target model’s hidden-state geometry, consistent with the hypothesis of a substantial common semantic subspace <d-cite key="huh2024platonic"></d-cite>.</p> <p>If you find this interesting and if this work is helpful for your research, please consider citing our paper:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">schiekiera2026associations</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{From Associations to Activations: Comparing Behavioral and
         Hidden-State Semantic Geometry in {LLMs}}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Schiekiera, Louis and Zimmer, Max and Roux, Christophe
          and Pokutta, Sebastian and G{\"u}nther, Fritz}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">eprint</span><span class="p">=</span><span class="s">{2602.00628}</span><span class="p">,</span>
  <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.LG}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2602.00628}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/neural_semantic_geometry.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Louis Schiekiera. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>